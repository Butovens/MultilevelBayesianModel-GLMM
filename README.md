# 1- Multilevel Bayesian modeling of ERP data

This is an analysis of ERP data from a large-scale replication project (Nieuwland et al., 2020) using a multilevel Bayesian model. The public_noun_data.csv file will be used. The paper and all data are available [here](https://osf.io/n5fkh/). We are interested in the relationship between cloze values and N400 amplitudes. We expect that this relationship may vary by subjects, items, and labs.

# 2- Generalized linear mixed model of lexical data

In the **long_short_data.csv** dataset, a group of researchers looked at word pairs like chimp/chimpanzee and math/mathematics. A prediction from information theory is that words that are predictable in context should be shorter. If you already know what the next word will be (as in “To be or not to. . . ”), there is no reason to spend a lot of time and effort saying a long word. On the other hand, if the word is entirely unpredictable (“The next word I am going to say is . . .”), the word should be longer and thus more robust to noise. That is, if one syllable gets garbled or misunderstood, the meaning can still be recovered. To test whether English is efficient in this regard, they gave subjects on Turk sentences like “Susan loves the apes at the zoo, and she even has a favorite . . . ” (supportive context) or “During a game of charades, Susan was too embarrassed to act like a . . . ” (neutral context) in which they were asked to complete the sentence with either the word chimp or chimpanzee.

There were 40 long-short word pairs, and they were presented to the subjects in random order such that each subject saw a different order of sentences. They also varied which form appeared first (the short one or the long one). They predicted that the shorter form would be more common in the predictable construction than in the unpredictable one for a whole list of such word pairs. The column PickedShort indicates whether they picked the short form (PickedShort = 1) or the long form (PickedShort = 0).
